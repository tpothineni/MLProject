{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_HateExplain.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpothineni/MLProject/blob/main/Example_HateExplain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Hqb1OvsXDR",
        "outputId": "21bf9e3c-27cc-4ab0-d0b8-87150e552e97"
      },
      "source": [
        "!git pull https://github.com/tpothineni/MLProject.git"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/tpothineni/MLProject\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPAJpqcYVEM0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4M1_SHtDIm",
        "outputId": "5fbea7e1-2ee6-4cc4-f243-85101ed7eaf3"
      },
      "source": [
        "cd MLProject/"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject/MLProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl3PF9alWjHx"
      },
      "source": [
        "!mkdir Saved/\n",
        "!mkdir explanations_dicts/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8P_dV5Wnk5",
        "outputId": "e5e1db55-b60e-4700-840e-bc1d9116d1e2"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip  -P Data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-09 17:36:34--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2024-11-09 17:36:34--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2024-11-09 17:36:35--  https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘Data/glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  4.95MB/s    in 6m 49s  \n",
            "\n",
            "2024-11-09 17:43:24 (4.38 MB/s) - ‘Data/glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZi9IlH-W_83",
        "outputId": "a7453d48-342e-4a22-ed97-ae468ee5caf0"
      },
      "source": [
        "!unzip Data/glove.42B.300d.zip -d Data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/glove.42B.300d.zip\n",
            "  inflating: Data/glove.42B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofRAO-crNgI"
      },
      "source": [
        "!rm Data/glove.42B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dkIaNCQDgvSP",
        "outputId": "b212d026-76ae-48b8-a829-8688dc6f32de"
      },
      "source": [
        "!pip install -r requirements.txt \"scikit_image >= 0.18.0\""
      ],
      "execution_count": 60,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit_image>=0.18.0 in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: apex==0.9.10dev in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.9.10.dev0)\n",
            "Collecting torch==1.11.0 (from -r requirements.txt (line 2))\n",
            "  Using cached torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting dataclasses==0.6 (from -r requirements.txt (line 3))\n",
            "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting GPUtil==1.4.0 (from -r requirements.txt (line 4))\n",
            "  Using cached GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit_learn==1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
            "Collecting scipy==1.13.0 (from -r requirements.txt (line 6))\n",
            "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting numpy==1.22.4 (from -r requirements.txt (line 7))\n",
            "  Using cached numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting spacy==3.8.2 (from -r requirements.txt (line 8))\n",
            "  Using cached spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting tqdm==4.43.0 (from -r requirements.txt (line 9))\n",
            "  Using cached tqdm-4.43.0-py2.py3-none-any.whl.metadata (49 kB)\n",
            "Collecting Keras==2.3.1 (from -r requirements.txt (line 10))\n",
            "  Using cached Keras-2.3.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting waiting==1.4.1 (from -r requirements.txt (line 11))\n",
            "  Using cached waiting-1.4.1.tar.gz (7.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ekphrasis==0.5.1 (from -r requirements.txt (line 12))\n",
            "  Using cached ekphrasis-0.5.1.tar.gz (80 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 13))\n",
            "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting transformers==2.5.1 (from -r requirements.txt (line 14))\n",
            "  Using cached transformers-2.5.1-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting lime==0.2.0.1 (from -r requirements.txt (line 15))\n",
            "  Using cached lime-0.2.0.1.tar.gz (275 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==3.2.1 (from -r requirements.txt (line 16))\n",
            "  Using cached matplotlib-3.2.1.tar.gz (40.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.3.3)\n",
            "Collecting neptune_client==0.4.107 (from -r requirements.txt (line 18))\n",
            "  Using cached neptune-client-0.4.107.tar.gz (86 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting knockknock==0.1.7 (from -r requirements.txt (line 19))\n",
            "  Using cached knockknock-0.1.7-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: cryptacular in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (1.6.2)\n",
            "Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (0.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.10/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 1)) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.5.2->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.5.2->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy==3.8.2->-r requirements.txt (line 8))\n",
            "  Using cached thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.2->-r requirements.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras==2.3.1->-r requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from Keras==2.3.1->-r requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from Keras==2.3.1->-r requirements.txt (line 10)) (3.12.1)\n",
            "Collecting keras-applications>=1.0.6 (from Keras==2.3.1->-r requirements.txt (line 10))\n",
            "  Using cached Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting keras-preprocessing>=1.0.5 (from Keras==2.3.1->-r requirements.txt (line 10))\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 12)) (2.5.0)\n",
            "Collecting colorama (from ekphrasis==0.5.1->-r requirements.txt (line 12))\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ujson (from ekphrasis==0.5.1->-r requirements.txt (line 12))\n",
            "  Using cached ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 12)) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis==0.5.1->-r requirements.txt (line 12))\n",
            "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 13)) (2024.2)\n",
            "Collecting tokenizers==0.5.2 (from transformers==2.5.1->-r requirements.txt (line 14))\n",
            "  Using cached tokenizers-0.5.2.tar.gz (64 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3 (from transformers==2.5.1->-r requirements.txt (line 14))\n",
            "  Using cached boto3-1.35.57-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.1->-r requirements.txt (line 14)) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.1->-r requirements.txt (line 14)) (2024.9.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.5.1->-r requirements.txt (line 14)) (0.2.0)\n",
            "Collecting sacremoses (from transformers==2.5.1->-r requirements.txt (line 14))\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 16)) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 16)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 16)) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3->-r requirements.txt (line 17)) (7.0.5)\n",
            "Collecting bravado (from neptune_client==0.4.107->-r requirements.txt (line 18))\n",
            "  Using cached bravado-11.0.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (1.0.0)\n",
            "Collecting py3nvml (from neptune_client==0.4.107->-r requirements.txt (line 18))\n",
            "  Using cached py3nvml-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (10.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (2.9.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (1.8.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 18)) (3.1.43)\n",
            "Collecting yagmail>=0.11.214 (from knockknock==0.1.7->-r requirements.txt (line 19))\n",
            "  Using cached yagmail-0.15.293-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: keyring in /usr/lib/python3/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 19)) (23.5.0)\n",
            "Collecting matrix-client (from knockknock==0.1.7->-r requirements.txt (line 19))\n",
            "  Using cached matrix_client-0.4.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-telegram-bot (from knockknock==0.1.7->-r requirements.txt (line 19))\n",
            "  Using cached python_telegram_bot-21.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting twilio (from knockknock==0.1.7->-r requirements.txt (line 19))\n",
            "  Using cached twilio-9.3.6-py2.py3-none-any.whl.metadata (12 kB)\n",
            "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit_image>=0.18.0\n",
            "  Using cached scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "  Using cached scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit_image>=0.18.0) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit_image>=0.18.0) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit_image>=0.18.0) (2024.9.20)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit_image>=0.18.0) (0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 18)) (4.0.11)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.8.2->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2->-r requirements.txt (line 8)) (2.23.4)\n",
            "Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: plaster in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (1.4)\n",
            "Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (1.8.9)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (5.0)\n",
            "Requirement already satisfied: zope.interface>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 1)) (7.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex==0.9.10dev->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex==0.9.10dev->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex==0.9.10dev->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex==0.9.10dev->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.3->-r requirements.txt (line 17)) (1.16.0)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy==3.8.2->-r requirements.txt (line 8))\n",
            "  Using cached blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy==3.8.2->-r requirements.txt (line 8)) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy==3.8.2->-r requirements.txt (line 8))\n",
            "  Using cached thinc-8.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "  Using cached thinc-8.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting keras-preprocessing>=1.0.5 (from Keras==2.3.1->-r requirements.txt (line 10))\n",
            "  Using cached Keras_Preprocessing-1.1.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is still looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached Keras_Preprocessing-1.0.9-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Using cached Keras_Preprocessing-1.0.8-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached Keras_Preprocessing-1.0.6-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Using cached Keras_Preprocessing-1.0.5-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting keras-applications>=1.0.6 (from Keras==2.3.1->-r requirements.txt (line 10))\n",
            "  Using cached Keras_Applications-1.0.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached Keras_Applications-1.0.6-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting imageio>=2.27 (from scikit_image>=0.18.0)\n",
            "  Using cached imageio-2.36.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.35.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.34.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.34.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.33.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.33.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.32.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting Pillow>=1.1.6 (from neptune_client==0.4.107->-r requirements.txt (line 18))\n",
            "  Using cached Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting imageio>=2.27 (from scikit_image>=0.18.0)\n",
            "  Using cached imageio-2.31.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Using cached imageio-2.31.5-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Using cached imageio-2.31.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Using cached imageio-2.31.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Using cached imageio-2.31.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.31.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.31.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.30.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.29.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.28.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached imageio-2.28.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached imageio-2.27.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting scikit_image>=0.18.0\n",
            "  Using cached scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit_image>=0.18.0)\n",
            "  Using cached pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "INFO: pip is still looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit_image>=0.18.0\n",
            "  Using cached scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting imageio>=2.4.1 (from scikit_image>=0.18.0)\n",
            "  Using cached imageio-2.26.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached imageio-2.26.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached imageio-2.25.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.25.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.24.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.23.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.22.4-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.22.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "  Using cached imageio-2.22.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.22.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.22.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.21.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.21.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.21.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.21.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.20.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.19.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.19.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.19.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.19.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.19.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "  Using cached imageio-2.18.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached imageio-2.17.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Using cached imageio-2.16.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached imageio-2.16.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached imageio-2.15.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "  Using cached imageio-2.14.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.14.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.13.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.11.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached imageio-2.10.5-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Using cached imageio-2.10.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Using cached imageio-2.10.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Using cached imageio-2.10.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Using cached imageio-2.10.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Using cached imageio-2.9.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached imageio-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached imageio-2.6.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached imageio-2.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached imageio-2.5.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached imageio-2.4.1.tar.gz (3.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit_image>=0.18.0\n",
            "  Using cached scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached scikit_image-0.19.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "  Using cached scikit_image-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "  Using cached scikit_image-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "  Using cached scikit-image-0.18.3.tar.gz (29.2 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio>=2.3.0 (from scikit_image>=0.18.0)\n",
            "  Using cached imageio-2.4.0.tar.gz (3.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached imageio-2.3.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting scikit_image>=0.18.0\n",
            "  Using cached scikit-image-0.18.2.tar.gz (30.8 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached scikit-image-0.18.1.tar.gz (29.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached scikit-image-0.18.0.tar.gz (36.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim==4.3.3 (from -r requirements.txt (line 17))\n",
            "  Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting scikit_learn==1.5.2 (from -r requirements.txt (line 5))\n",
            "  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "\u001b[31mERROR: Cannot install -r requirements.txt (line 10), -r requirements.txt (line 12), -r requirements.txt (line 13), -r requirements.txt (line 14), -r requirements.txt (line 15), -r requirements.txt (line 16), -r requirements.txt (line 17), -r requirements.txt (line 5), -r requirements.txt (line 6), -r requirements.txt (line 8), numpy==1.22.4, scikit-image==0.23.1, scikit-image==0.23.2 and scikit-image==0.24.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested numpy==1.22.4\n",
            "    scikit-learn 1.5.2 depends on numpy>=1.19.5\n",
            "    scipy 1.13.0 depends on numpy<2.3 and >=1.22.4\n",
            "    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= \"3.9\"\n",
            "    keras 2.3.1 depends on numpy>=1.9.1\n",
            "    ekphrasis 0.5.1 depends on numpy\n",
            "    pandas 2.2.3 depends on numpy>=1.22.4; python_version < \"3.11\"\n",
            "    transformers 2.5.1 depends on numpy\n",
            "    lime 0.2.0.1 depends on numpy\n",
            "    matplotlib 3.2.1 depends on numpy>=1.11\n",
            "    gensim 4.3.3 depends on numpy<2.0 and >=1.18.5\n",
            "    scikit-image 0.24.0 depends on numpy>=1.23\n",
            "    The user requested numpy==1.22.4\n",
            "    scikit-learn 1.5.2 depends on numpy>=1.19.5\n",
            "    scipy 1.13.0 depends on numpy<2.3 and >=1.22.4\n",
            "    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= \"3.9\"\n",
            "    keras 2.3.1 depends on numpy>=1.9.1\n",
            "    ekphrasis 0.5.1 depends on numpy\n",
            "    pandas 2.2.3 depends on numpy>=1.22.4; python_version < \"3.11\"\n",
            "    transformers 2.5.1 depends on numpy\n",
            "    lime 0.2.0.1 depends on numpy\n",
            "    matplotlib 3.2.1 depends on numpy>=1.11\n",
            "    gensim 4.3.3 depends on numpy<2.0 and >=1.18.5\n",
            "    scikit-image 0.23.2 depends on numpy>=1.23\n",
            "    The user requested numpy==1.22.4\n",
            "    scikit-learn 1.5.2 depends on numpy>=1.19.5\n",
            "    scipy 1.13.0 depends on numpy<2.3 and >=1.22.4\n",
            "    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= \"3.9\"\n",
            "    keras 2.3.1 depends on numpy>=1.9.1\n",
            "    ekphrasis 0.5.1 depends on numpy\n",
            "    pandas 2.2.3 depends on numpy>=1.22.4; python_version < \"3.11\"\n",
            "    transformers 2.5.1 depends on numpy\n",
            "    lime 0.2.0.1 depends on numpy\n",
            "    matplotlib 3.2.1 depends on numpy>=1.11\n",
            "    gensim 4.3.3 depends on numpy<2.0 and >=1.18.5\n",
            "    scikit-image 0.23.1 depends on numpy>=1.23\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlq-y7f7nxdP"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVyCVHdlWSc",
        "outputId": "24615b10-bb27-47e7-881c-b340760f103d"
      },
      "source": [
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HS3NNHzmX64"
      },
      "source": [
        "!rm Data/glove.42B.300d.txt\n",
        "!rm Data/glove.42B.300d_w2v.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "53QcDc7AqkK5",
        "outputId": "11b6eba6-6096-4334-dfdc-ab9b176acba7"
      },
      "source": [
        "from manual_training_inference import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'neptune'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-04a9809c1776>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmanual_training_inference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/MLProject/manual_training_inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mknockknock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslack_sender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neptune'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKH2h5hzwcT"
      },
      "source": [
        "path_file='best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file,mode='r') as f:\n",
        "    params = json.load(f)\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "          params[key]=True\n",
        "    elif params[key] == 'False':\n",
        "          params[key]=False\n",
        "    if( key in ['batch_size','num_classes','hidden_size','supervised_layer_pos','num_supervised_heads','random_seed','max_length']):\n",
        "        if(params[key]!='N/A'):\n",
        "            params[key]=int(params[key])\n",
        "\n",
        "    if((key == 'weights') and (params['auto_weights']==False)):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "##### change in logging to output the results to neptune\n",
        "params['logging']='local'\n",
        "params['device']='cpu'\n",
        "params['best_params']=False\n",
        "\n",
        "if torch.cuda.is_available() and params['device']=='cuda':\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Since you dont want to use GPU, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "#### Few handy keys that you can directly change.\n",
        "params['variance']=1\n",
        "params['epochs']=5\n",
        "params['to_save']=True\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "\n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBj47ArF8-F"
      },
      "source": [
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "\n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkvPokoMg3f"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34jLhxdQ5stq"
      },
      "source": [
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SynOxIW5PY-v"
      },
      "source": [
        "!ls explanations_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DVEb9O3IlCd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Ka6ukjTC5M"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-wjKhvNrF5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Bias Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPagtSDQ9zo"
      },
      "source": [
        "from collections import Counter,defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFOmyTVIRJ7R"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw2qaVXLROW4"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'.\n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRrrN8CRRfg"
      },
      "source": [
        "data_all_labelled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14K9-nzFRU8B"
      },
      "source": [
        "def generate_target_information(dataset):\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "\n",
        "    for each in dataset.iterrows():\n",
        "        # All the target communities tagged for this post\n",
        "        all_targets = each[1]['target1']+each[1]['target2']+each[1]['target3']\n",
        "        community_dict = dict(Counter(all_targets))\n",
        "\n",
        "        # Select only those communities which are present more than once.\n",
        "        for key in community_dict:\n",
        "            if community_dict[key]>1:\n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "\n",
        "        # If no community is selected based on majority voting then we don't select any community\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append(key)\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEmm7AD9Ra13"
      },
      "source": [
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPCh_pj3ReFA"
      },
      "source": [
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# We remove None and Other from dictionary\n",
        "community_count_dict.pop('None')\n",
        "community_count_dict.pop('Other')\n",
        "\n",
        "# For the bias calculation, we are considering the top 10 communites based on their count\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "list_selected_community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRU3peSrRhVo"
      },
      "source": [
        "# Based on the top 10 communities, we filter the target_information\n",
        "# This will remove the other communities from the calculation\n",
        "\n",
        "final_target_information ={}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each])&set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsvl0xaRkpw"
      },
      "source": [
        "# Add a new column 'final_target_category' which will contain the selected target community names\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-Y6iMiRnxH"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "postpost_id_divisions_path = './Data/post_id_divisions.json'\n",
        "\n",
        "with open(postpost_id_divisions_path, 'r') as fp:\n",
        "    post_id_dict=json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "data_all_labelled_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT3uzWgcRqaf"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# The names of the files which will contain the bias output for each model\n",
        "bias_score_file_mapping={\n",
        "    'BiRNN-Attn':'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "# The parent folder path of the bias output files\n",
        "parent_path = './explanations_dicts/'\n",
        "\n",
        "# The bias methods that will be considered\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "community_list = list(list_selected_community)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0H9nMZNR1v3"
      },
      "source": [
        "# This function is used to convert the classification into a [0-1] score\n",
        "# with a value of 0 meaning non-toxic and 1 meaning toxic\n",
        "def convert_to_score(label_name, label_dict):\n",
        "    if label_name=='non-toxic':\n",
        "        return 1-label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFyY8iVR4kn"
      },
      "source": [
        "# Function to divide the ids into postive or class class based on the method.\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "    if method=='subgroup':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "            else:\n",
        "                pass\n",
        "    elif method=='bpsn':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "#                 print(eachrow[1]['final_label'])\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    elif method=='bnsp':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        print('Incorrect option selected!!!')\n",
        "\n",
        "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-CBxRFR7YQ"
      },
      "source": [
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "# We load each of the model bias output file and compute the bias score using each method for all the community\n",
        "for each_model in tqdm(bias_score_file_mapping):\n",
        "    total_data ={}\n",
        "    with open(parent_path+bias_score_file_mapping[each_model]) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "\n",
        "            label_to_value = {'toxic':1.0, 'non-toxic':0.0}\n",
        "            for each in community_data['positiveID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "            final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uoz4r8JR-23"
      },
      "source": [
        "%precision 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQi-Am9SCIz"
      },
      "source": [
        "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value =[]\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        print(each_model, each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1-xKrKSFza"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos2FyRyScI6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Calculate Explainability**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0n04ccES0G3"
      },
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ37OLVZS8gB"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess import *\n",
        "from Preprocess.dataCollect import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AZCJG3wS-ko"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class).\n",
        "\n",
        "params = {}\n",
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5DLppxPTAjo"
      },
      "source": [
        "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
        "\n",
        "params_data={\n",
        "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
        "    'bert_tokens':False, #True /False\n",
        "    'type_attention':'softmax', #softmax\n",
        "    'set_decay':0.1,\n",
        "    'majority':2,\n",
        "    'max_length':128,\n",
        "    'variance':10,\n",
        "    'window':4,\n",
        "    'alpha':0.5,\n",
        "    'p_value':0.8,\n",
        "    'method':'additive',\n",
        "    'decay':False,\n",
        "    'normalized':False,\n",
        "    'not_recollect':True,\n",
        "}\n",
        "\n",
        "\n",
        "if(params_data['bert_tokens']):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Loading Normal tokenizer...')\n",
        "    tokenizer=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqwkmy9ITEvH"
      },
      "source": [
        "# Load the whole dataset and get the tokenwise rationales\n",
        "def get_training_data(data):\n",
        "    post_ids_list=[]\n",
        "    text_list=[]\n",
        "    attention_list=[]\n",
        "    label_list=[]\n",
        "\n",
        "    final_binny_output = []\n",
        "    print('total_data',len(data))\n",
        "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
        "        annotation=row['final_label']\n",
        "\n",
        "        text=row['text']\n",
        "        post_id=row['post_id']\n",
        "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
        "        tokens_all = list(row['text'])\n",
        "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
        "\n",
        "        if(annotation!= 'undecided'):\n",
        "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
        "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_binny_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lkIo1ATHjH"
      },
      "source": [
        "training_data=get_training_data(data_all_labelled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxfQUvdTJzn"
      },
      "source": [
        "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    output = []\n",
        "\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if type(each)== int:\n",
        "            start = each\n",
        "            end = each+1\n",
        "        elif len(each) == 2:\n",
        "            start = each[0]\n",
        "            end = each[1]+1\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        output.append({\"docid\":post_id,\n",
        "              \"end_sentence\": -1,\n",
        "              \"end_token\": end,\n",
        "              \"start_sentence\": -1,\n",
        "              \"start_token\": start,\n",
        "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
        "    return output\n",
        "\n",
        "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
        "    final_output = []\n",
        "\n",
        "    if save_split:\n",
        "        train_fp = open(save_path+'train.jsonl', 'w')\n",
        "        val_fp = open(save_path+'val.jsonl', 'w')\n",
        "        test_fp = open(save_path+'test.jsonl', 'w')\n",
        "\n",
        "    for tcount, eachrow in enumerate(dataset):\n",
        "\n",
        "        temp = {}\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "        majority_label = eachrow[1]\n",
        "\n",
        "        if majority_label=='normal':\n",
        "            continue\n",
        "\n",
        "        all_labels = eachrow[4]\n",
        "        explanations = []\n",
        "        for each_explain in eachrow[3]:\n",
        "            explanations.append(list(each_explain))\n",
        "\n",
        "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
        "        if method == 'union':\n",
        "            final_explanation = [any(each) for each in zip(*explanations)]\n",
        "            final_explanation = [int(each) for each in final_explanation]\n",
        "\n",
        "\n",
        "        temp['annotation_id'] = post_id\n",
        "        temp['classification'] = post_class\n",
        "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
        "        temp['query'] = \"What is the class?\"\n",
        "        temp['query_type'] = None\n",
        "        final_output.append(temp)\n",
        "\n",
        "        if save_split:\n",
        "            if not os.path.exists(save_path+'docs'):\n",
        "                os.makedirs(save_path+'docs')\n",
        "\n",
        "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "\n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp)+'\\n')\n",
        "\n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp)+'\\n')\n",
        "\n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp)+'\\n')\n",
        "            else:\n",
        "                print(post_id)\n",
        "\n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    return final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deYKnU3wTRJn"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "with open('./Data/post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlA0iMeETjUd"
      },
      "source": [
        "!mkdir ./Data/Evaluation\n",
        "!mkdir ./Data/Evaluation/Model_Eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XwjEPOTUaY"
      },
      "source": [
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e76FcTICTXrX"
      },
      "source": [
        "!ls Data/Evaluation/Model_Eval/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9lpwdAeTaf_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWLQB2uT28g"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylHvxsmoUv7Q"
      },
      "source": [
        "cd eraserbenchmark/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajAK6cMUyt6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iudyL6lcXib"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZLhJf-U8Vf"
      },
      "source": [
        "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test  --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json --score_file ../model_explain_output.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1eQENR4VLp_"
      },
      "source": [
        "# print the required results\n",
        "with open('../model_explain_output.json') as fp:\n",
        "    output_data = json.load(fp)\n",
        "\n",
        "print('\\nPlausibility')\n",
        "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
        "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
        "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
        "\n",
        "print('\\nFaithfulness')\n",
        "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
        "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND6DYOMxTU8A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}