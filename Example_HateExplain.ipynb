{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_HateExplain.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpothineni/MLProject/blob/main/Example_HateExplain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Hqb1OvsXDR",
        "outputId": "ebc75db4-1d35-4e30-807f-975da5ffc42a"
      },
      "source": [
        "!git clone https://github.com/tpothineni/MLProject.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLProject'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 161 (delta 67), reused 89 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (161/161), 2.38 MiB | 3.30 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPAJpqcYVEM0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4M1_SHtDIm",
        "outputId": "ec58b6db-732b-4446-958f-ba37be1d636b"
      },
      "source": [
        "cd MLProject/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MLProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl3PF9alWjHx"
      },
      "source": [
        "!mkdir Saved/\n",
        "!mkdir explanations_dicts/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8P_dV5Wnk5",
        "outputId": "3b7196bb-7083-42a1-fdd8-bcd1af2a03b1"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip  -P Data/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 22:53:24--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2024-11-11 22:53:24--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2024-11-11 22:53:24--  https://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘Data/glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  5.00MB/s    in 5m 52s  \n",
            "\n",
            "2024-11-11 22:59:17 (5.08 MB/s) - ‘Data/glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZi9IlH-W_83",
        "outputId": "02649df0-f827-4dff-803a-053dc8c6062a"
      },
      "source": [
        "!unzip Data/glove.42B.300d.zip -d Data/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/glove.42B.300d.zip\n",
            "  inflating: Data/glove.42B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofRAO-crNgI"
      },
      "source": [
        "!rm Data/glove.42B.300d.zip"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dkIaNCQDgvSP",
        "outputId": "fc1051f7-2bed-465b-a8f6-19d70291448c"
      },
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8 python3.8-dev python3.8-venv -y\n",
        "!sudo apt-get install libatlas-base-dev\n",
        "# Step 2: Create a virtual environment with Python 3.8\n",
        "!python3.8 -m venv myenv\n",
        "\n",
        "# Step 3: Activate the virtual environment (specific to Colab)\n",
        "!source myenv/bin/activate && echo \"Virtual environment activated\"\n",
        "\n",
        "# Step 4: Upgrade pip within the virtual environment\n",
        "!source myenv/bin/activate && pip install --upgrade pip\n",
        "\n",
        "# Step 5: Install dependencies from requirements.txt within the environment\n",
        "!source myenv/bin/activate && pip install -r requirements.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Connected to clou\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 12.7 kB/128 kB 10%] [Connecting to security.ubuntu.com (185.125\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [Connecting to security.ubuntu.com (185.125\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [2 InRelease 117 kB/128 kB 91%] [Connecting to security.ubuntu.com (185.125.\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,700 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,608 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,457 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,422 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,164 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,269 kB]\n",
            "Fetched 22.5 MB in 3s (8,118 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib\n",
            "  mailcap mime-support python3.8-distutils python3.8-lib2to3 python3.8-minimal\n",
            "Suggested packages:\n",
            "  binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib\n",
            "  mailcap mime-support python3.8 python3.8-dev python3.8-distutils\n",
            "  python3.8-lib2to3 python3.8-minimal python3.8-venv\n",
            "0 upgraded, 12 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 14.7 MB of archives.\n",
            "After this operation, 48.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.20-1+jammy1 [796 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.20-1+jammy1 [2,023 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.20-1+jammy1 [1,817 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8 amd64 3.8.20-1+jammy1 [1,798 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-dev amd64 3.8.20-1+jammy1 [4,389 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.20-1+jammy1 [440 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-dev amd64 3.8.20-1+jammy1 [500 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.20-1+jammy1 [126 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.20-1+jammy1 [193 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-venv amd64 3.8.20-1+jammy1 [2,618 kB]\n",
            "Fetched 14.7 MB in 2s (8,883 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 12.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 123624 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../01-python3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../02-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../03-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../04-libpython3.8-stdlib_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.8:amd64.\n",
            "Preparing to unpack .../05-libpython3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.8-dev:amd64.\n",
            "Preparing to unpack .../06-libpython3.8-dev_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-dev:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../07-python3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-dev.\n",
            "Preparing to unpack .../08-python3.8-dev_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-dev (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "Preparing to unpack .../09-python3.8-lib2to3_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../10-python3.8-distutils_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-venv.\n",
            "Preparing to unpack .../11-python3.8-venv_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-venv (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-dev:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-dev (3.8.20-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libatlas-base-dev is already the newest version (3.10.3-12ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Virtual environment activated\n",
            "Requirement already satisfied: pip in ./myenv/lib/python3.8/site-packages (23.0.1)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.0.1\n",
            "    Uninstalling pip-23.0.1:\n",
            "      Successfully uninstalled pip-23.0.1\n",
            "Successfully installed pip-24.3.1\n",
            "Collecting scipy==1.4.1 (from -r requirements.txt (line 1))\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting spacy==2.3.2 (from -r requirements.txt (line 2))\n",
            "  Downloading spacy-2.3.2-cp38-cp38-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Collecting tqdm==4.43.0 (from -r requirements.txt (line 3))\n",
            "  Downloading tqdm-4.43.0-py2.py3-none-any.whl.metadata (49 kB)\n",
            "Collecting Keras==2.3.1 (from -r requirements.txt (line 4))\n",
            "  Downloading Keras-2.3.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting waiting==1.4.1 (from -r requirements.txt (line 5))\n",
            "  Downloading waiting-1.4.1.tar.gz (7.1 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ekphrasis==0.5.1 (from -r requirements.txt (line 6))\n",
            "  Downloading ekphrasis-0.5.1.tar.gz (80 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==1.0.3 (from -r requirements.txt (line 7))\n",
            "  Downloading pandas-1.0.3-cp38-cp38-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting transformers==2.5.1 (from -r requirements.txt (line 8))\n",
            "  Downloading transformers-2.5.1-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting lime==0.2.0.1 (from -r requirements.txt (line 9))\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.19.5 (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting matplotlib==3.2.1 (from -r requirements.txt (line 11))\n",
            "  Downloading matplotlib-3.2.1-cp38-cp38-manylinux1_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting gensim==3.8.1 (from -r requirements.txt (line 12))\n",
            "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting neptune_client==0.4.107 (from -r requirements.txt (line 13))\n",
            "  Downloading neptune-client-0.4.107.tar.gz (86 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting knockknock==0.1.7 (from -r requirements.txt (line 14))\n",
            "  Downloading knockknock-0.1.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting torch==1.4.0 (from -r requirements.txt (line 15))\n",
            "  Downloading torch-1.4.0-cp38-cp38-manylinux1_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting apex==0.9.10dev (from -r requirements.txt (line 16))\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting GPUtil==1.4.0 (from -r requirements.txt (line 17))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit_learn==0.23.2 (from -r requirements.txt (line 18))\n",
            "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc==7.4.1 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading thinc-7.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (22 kB)\n",
            "Collecting blis<0.5.0,>=0.4.0 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading srsly-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in ./myenv/lib/python3.8/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (56.0.0)\n",
            "Collecting plac<1.2.0,>=0.9.6 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting six>=1.9.0 (from Keras==2.3.1->-r requirements.txt (line 4))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyyaml (from Keras==2.3.1->-r requirements.txt (line 4))\n",
            "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting h5py (from Keras==2.3.1->-r requirements.txt (line 4))\n",
            "  Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting keras-applications>=1.0.6 (from Keras==2.3.1->-r requirements.txt (line 4))\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting keras-preprocessing>=1.0.5 (from Keras==2.3.1->-r requirements.txt (line 4))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting termcolor (from ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting colorama (from ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ujson (from ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading ujson-5.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting nltk (from ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting ftfy (from ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting python-dateutil>=2.6.1 (from pandas==1.0.3->-r requirements.txt (line 7))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2017.2 (from pandas==1.0.3->-r requirements.txt (line 7))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tokenizers==0.5.2 (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading tokenizers-0.5.2-cp38-cp38-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting boto3 (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading boto3-1.35.58-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting filelock (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting sentencepiece (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting sacremoses (from transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting scikit-image>=0.12 (from lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading scikit_image-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib==3.2.1->-r requirements.txt (line 11))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib==3.2.1->-r requirements.txt (line 11))\n",
            "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib==3.2.1->-r requirements.txt (line 11))\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim==3.8.1->-r requirements.txt (line 12))\n",
            "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting bravado (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting click>=7.0 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting future>=0.17.1 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting py3nvml (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting oauthlib>=2.1.0 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting Pillow>=1.1.6 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting PyJWT (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests-oauthlib>=1.0.0 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting websocket-client>=0.35.0 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting GitPython>=2.0.8 (from neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting yagmail>=0.11.214 (from knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading yagmail-0.15.293-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting keyring (from knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading keyring-25.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting matrix-client (from knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading matrix_client-0.4.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-telegram-bot (from knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading python_telegram_bot-21.6-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting twilio (from knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading twilio-9.3.6-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting cryptacular (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading zope.sqlalchemy-3.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting velruse>=1.0.3 (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyramid>1.1.2 (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading pyramid-2.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pyramid-mailer (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting wtforms (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting wtforms-recaptcha (from apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting joblib>=0.11 (from scikit_learn==0.23.2->-r requirements.txt (line 18))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit_learn==0.23.2->-r requirements.txt (line 18))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting hupper>=1.5 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading hupper-1.12.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting plaster (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading plaster-1.1.2-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting plaster-pastedeploy (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting translationstring>=0.4 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting venusian>=1.0 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading venusian-3.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting webob>=1.8.3 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading WebOb-1.8.9-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting zope.deprecation>=3.5.0 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading zope.deprecation-5.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting zope.interface>=3.8.0 (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading zope.interface-7.1.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading charset_normalizer-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-image>=0.12 (from lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading scikit_image-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting networkx>=2.2 (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting imageio>=2.4.1 (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tifffile>=2019.7.26 (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting packaging>=20.0 (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9))\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 12))\n",
            "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting anykeystore (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-openid (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting premailer (from yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading premailer-3.10.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.58 (from boto3->transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading botocore-1.35.58-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->transformers==2.5.1->-r requirements.txt (line 8))\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting bravado-core>=5.16.1 (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading msgpack-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting simplejson (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading simplejson-3.19.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting monotonic (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting typing-extensions (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pbkdf2 (from cryptacular->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->ekphrasis==0.5.1->-r requirements.txt (line 6))\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting jaraco.classes (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jaraco.functools (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting jaraco.context (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting importlib-metadata>=4.11.4 (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting importlib-resources (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting SecretStorage>=3.2 (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading SecretStorage-3.3.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting jeepney>=0.4.2 (from keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading jeepney-0.8.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "Collecting xmltodict (from py3nvml->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting repoze.sendmail>=4.1 (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting transaction (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading transaction-5.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting httpx~=0.27 (from python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting aiohttp>=3.8.4 (from twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading aiohttp-3.10.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting markupsafe (from wtforms->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 (from zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting jsonschema>=2.5.1 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting swagger-spec-validator>=2.0.1 (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting anyio (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading anyio-4.5.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting httpcore==1.* (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sniffio (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx~=0.27->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata>=4.11.4->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting cryptography>=2.0 (from SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading cryptography-43.0.3-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting more-itertools (from jaraco.classes->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting backports.tarfile (from jaraco.context->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PasteDeploy>=2.0 (from plaster-pastedeploy->pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading PasteDeploy-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting lxml (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading lxml-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting cssselect (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cssutils (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading cssutils-2.11.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting cachetools (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting defusedxml (from python3-openid->velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16))\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting cffi>=1.12 (from cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading cffi-1.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rfc3339-validator (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading webcolors-24.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp>=3.8.4->twilio->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio->httpx~=0.27->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pycparser (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14))\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13))\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-2.3.2-cp38-cp38-manylinux1_x86_64.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
            "Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
            "Downloading pandas-1.0.3-cp38-cp38-manylinux1_x86_64.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-2.5.1-py3-none-any.whl (499 kB)\n",
            "Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.2.1-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading knockknock-0.1.7-py3-none-any.whl (19 kB)\n",
            "Downloading torch-1.4.0-cp38-cp38-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-7.4.1-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.5.2-cp38-cp38-manylinux1_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "Downloading pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
            "Downloading srsly-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (370 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading yagmail-0.15.293-py2.py3-none-any.whl (17 kB)\n",
            "Downloading boto3-1.35.58-py3-none-any.whl (139 kB)\n",
            "Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keyring-25.5.0-py3-none-any.whl (39 kB)\n",
            "Downloading matrix_client-0.4.0-py2.py3-none-any.whl (43 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_telegram_bot-21.6-py3-none-any.whl (652 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m652.1/652.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.5/746.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading twilio-9.3.6-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
            "Downloading aiohttp-3.10.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading botocore-1.35.58-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Downloading charset_normalizer-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "Downloading hupper-1.12.1-py3-none-any.whl (22 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
            "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading msgpack-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
            "Downloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
            "Downloading transaction-5.0-py3-none-any.whl (46 kB)\n",
            "Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "Downloading venusian-3.1.0-py3-none-any.whl (13 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Downloading WebOb-1.8.9-py2.py3-none-any.whl (115 kB)\n",
            "Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
            "Downloading zope.interface-7.1.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (257 kB)\n",
            "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "Downloading simplejson-3.19.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "Downloading cryptography-43.0.3-cp37-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
            "Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.0/606.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Downloading PasteDeploy-3.1.0-py3-none-any.whl (16 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Downloading anyio-4.5.2-py3-none-any.whl (89 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading cssutils-2.11.1-py3-none-any.whl (385 kB)\n",
            "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading lxml-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
            "Downloading cffi-1.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
            "Downloading webcolors-24.8.0-py3-none-any.whl (15 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: waiting, ekphrasis, lime, gensim, neptune_client, apex, GPUtil, velruse, cryptacular, bravado-core, anykeystore, pbkdf2\n",
            "  Building wheel for waiting (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for waiting: filename=waiting-1.4.1-py3-none-any.whl size=3735 sha256=a0b16330ac3630ce1b3e7220e6c14b5be6ee1ac35792ccea82924535eb815034\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/73/26/87317a55070b56e3b102aa208b0459c9265d889ecdefd5bcb9\n",
            "  Building wheel for ekphrasis (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-py3-none-any.whl size=82827 sha256=be75f6f37ee280abd7bca6231357a7a0d4741941e3ecaae8daab15c2f01ec42b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/36/c7/477ab8220745f8d1f84b56de0ab88544b410f80a830f6feee6\n",
            "  Building wheel for lime (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=2aa3aa3434e7be2ba7da66318ea51784cdb713e079285d63210357d0eb3ca73d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/a6/20/cc1e293fcdb67ede666fed293cb895395e7ecceb4467779546\n",
            "  Building wheel for gensim (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.8.1-cp38-cp38-linux_x86_64.whl size=26122769 sha256=f9e998e58b9db2a4a1abad08c4e15bef2b5beecb28e962127e4c0dd91d550276\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/de/03/7346ae70da7f980f78569668caf78fb2d678b176e549557c7d\n",
            "  Building wheel for neptune_client (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune_client: filename=neptune_client-0.4.107-py2.py3-none-any.whl size=145039 sha256=8684359f8a32222d1fe50730e55afd3f94123c7558c2782c85006d88ae16a743\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/c0/ee/9f671eea5e609f5b278933d9bfecef8fbfff3c712463cf9faa\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46444 sha256=8f03bdc58b45ecd1cf2fc8f232c88afbdb9685c79914fed4f3bdf3f210b403b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/6a/6f/95af6942b4f627ec422765dcffb665a000b8432c982ff67045\n",
            "  Building wheel for GPUtil (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=213e20f09b2fdb30000857832ed56ece1490f4364b3d12bf80a3800cb5c9a471\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/03/bb/7a97840eb54479b328672e15a536e49dc60da200fb21564d53\n",
            "  Building wheel for velruse (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50908 sha256=78f04848079a46f120aedea6b62f8d4f8660d8f873b84c88ad910ce23cb702db\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/75/d1/acf6bc66263e644c145ee1ed5e9ed3f1225043550ad506d4a7\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp38-cp38-linux_x86_64.whl size=55346 sha256=e595b5f1079f8de3f85d6cd740b8f3ad3cf606c8f141c3ebd596472098d95582\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/0e/3f/ae80d1aba7da71d30038142c4c6922b05b03ba15df20e40143\n",
            "  Building wheel for bravado-core (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67673 sha256=8ebd6ef7364f14caad39babe956a8e7d94d14afdd5951b837336c5d2bc67f5e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/16/68/7f90b317dc2d4440d00bd692958b3dc8ae05252c5c93d48c9a\n",
            "  Building wheel for anykeystore (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=16813 sha256=d8943462a9a088b027b2b240c944ff733aa478b9046b4d89cfc968c40d63ad2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/b2/a2/7e7725cfa204e1d7b54ba5fa2fcccd01c666353d61d9fb00c0\n",
            "  Building wheel for pbkdf2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5082 sha256=54077e5920a651c1aa7a4e0b3db13dcf8066814a9c8d0bb74730e173ce66c279\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/bc/b5/384fdb7fc125e4d3d25b92eb8365b87c8b652a7096fcdcc16e\n",
            "Successfully built waiting ekphrasis lime gensim neptune_client apex GPUtil velruse cryptacular bravado-core anykeystore pbkdf2\n",
            "Installing collected packages: wcwidth, wasabi, waiting, translationstring, tokenizers, sentencepiece, pytz, plac, pbkdf2, monotonic, GPUtil, cymem, anykeystore, zope.interface, zope.deprecation, zipp, xmltodict, wrapt, websocket-client, webob, webcolors, venusian, urllib3, uri-template, ujson, typing-extensions, types-python-dateutil, tqdm, torch, threadpoolctl, termcolor, srsly, sniffio, smmap, six, simplejson, rpds-py, rfc3986-validator, regex, pyyaml, pyparsing, PyJWT, pycparser, propcache, plaster, pkgutil-resolve-name, Pillow, PasteDeploy, packaging, oauthlib, numpy, networkx, murmurhash, msgpack, more-itertools, markupsafe, lxml, kiwisolver, jsonref, jsonpointer, joblib, jmespath, jeepney, idna, hupper, h11, greenlet, future, ftfy, frozenlist, fqdn, filelock, exceptiongroup, defusedxml, cycler, cssselect, cryptacular, colorama, click, charset-normalizer, certifi, catalogue, cachetools, backports.tarfile, attrs, async-timeout, aiohappyeyeballs, wtforms, transaction, tifffile, SQLAlchemy, smart-open, scipy, sacremoses, rfc3339-validator, requests, referencing, PyWavelets, python3-openid, python-dateutil, py3nvml, preshed, plaster-pastedeploy, nltk, multidict, keras-preprocessing, jaraco.functools, jaraco.context, jaraco.classes, importlib-resources, importlib-metadata, imageio, httpcore, h5py, gitdb, cssutils, cffi, blis, anyio, aiosignal, zope.sqlalchemy, yarl, wtforms-recaptcha, thinc, scikit_learn, scikit-image, requests-oauthlib, repoze.sendmail, pyramid, premailer, pandas, matrix-client, matplotlib, keras-applications, jsonschema-specifications, httpx, GitPython, gensim, cryptography, botocore, arrow, yagmail, velruse, spacy, SecretStorage, s3transfer, python-telegram-bot, pyramid-mailer, lime, Keras, jsonschema, isoduration, ekphrasis, aiohttp, swagger-spec-validator, keyring, boto3, apex, aiohttp-retry, twilio, transformers, bravado-core, knockknock, bravado, neptune_client\n",
            "Successfully installed GPUtil-1.4.0 GitPython-3.1.43 Keras-2.3.1 PasteDeploy-3.1.0 Pillow-10.4.0 PyJWT-2.9.0 PyWavelets-1.4.1 SQLAlchemy-2.0.36 SecretStorage-3.3.3 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiohttp-retry-2.9.1 aiosignal-1.3.1 anyio-4.5.2 anykeystore-0.2 apex-0.9.10.dev0 arrow-1.3.0 async-timeout-4.0.3 attrs-24.2.0 backports.tarfile-1.2.0 blis-0.4.1 boto3-1.35.58 botocore-1.35.58 bravado-11.0.3 bravado-core-6.1.1 cachetools-5.5.0 catalogue-1.0.2 certifi-2024.8.30 cffi-1.17.1 charset-normalizer-3.4.0 click-8.1.7 colorama-0.4.6 cryptacular-1.6.2 cryptography-43.0.3 cssselect-1.2.0 cssutils-2.11.1 cycler-0.12.1 cymem-2.0.8 defusedxml-0.7.1 ekphrasis-0.5.1 exceptiongroup-1.2.2 filelock-3.16.1 fqdn-1.5.1 frozenlist-1.5.0 ftfy-6.2.3 future-1.0.0 gensim-3.8.1 gitdb-4.0.11 greenlet-3.1.1 h11-0.14.0 h5py-3.11.0 httpcore-1.0.6 httpx-0.27.2 hupper-1.12.1 idna-3.10 imageio-2.35.1 importlib-metadata-8.5.0 importlib-resources-6.4.5 isoduration-20.11.0 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jeepney-0.8.0 jmespath-1.0.1 joblib-1.4.2 jsonpointer-3.0.0 jsonref-1.1.0 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 keyring-25.5.0 kiwisolver-1.4.7 knockknock-0.1.7 lime-0.2.0.1 lxml-5.3.0 markupsafe-2.1.5 matplotlib-3.2.1 matrix-client-0.4.0 monotonic-1.6 more-itertools-10.5.0 msgpack-1.1.0 multidict-6.1.0 murmurhash-1.0.10 neptune_client-0.4.107 networkx-3.1 nltk-3.9.1 numpy-1.19.5 oauthlib-3.2.2 packaging-24.2 pandas-1.0.3 pbkdf2-1.3 pkgutil-resolve-name-1.3.10 plac-1.1.3 plaster-1.1.2 plaster-pastedeploy-1.0.1 premailer-3.10.0 preshed-3.0.9 propcache-0.2.0 py3nvml-0.2.7 pycparser-2.22 pyparsing-3.1.4 pyramid-2.0.2 pyramid-mailer-0.15.1 python-dateutil-2.9.0.post0 python-telegram-bot-21.6 python3-openid-3.2.0 pytz-2024.2 pyyaml-6.0.2 referencing-0.35.1 regex-2024.11.6 repoze.sendmail-4.4.1 requests-2.32.3 requests-oauthlib-2.0.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.20.1 s3transfer-0.10.3 sacremoses-0.1.1 scikit-image-0.19.3 scikit_learn-0.23.2 scipy-1.4.1 sentencepiece-0.2.0 simplejson-3.19.3 six-1.16.0 smart-open-7.0.5 smmap-5.0.1 sniffio-1.3.1 spacy-2.3.2 srsly-1.0.7 swagger-spec-validator-3.0.4 termcolor-2.4.0 thinc-7.4.1 threadpoolctl-3.5.0 tifffile-2023.7.10 tokenizers-0.5.2 torch-1.4.0 tqdm-4.43.0 transaction-5.0 transformers-2.5.1 translationstring-1.4 twilio-9.3.6 types-python-dateutil-2.9.0.20241003 typing-extensions-4.12.2 ujson-5.10.0 uri-template-1.3.0 urllib3-1.26.20 velruse-1.1.1 venusian-3.1.0 waiting-1.4.1 wasabi-0.10.1 wcwidth-0.2.13 webcolors-24.8.0 webob-1.8.9 websocket-client-1.8.0 wrapt-1.16.0 wtforms-3.1.2 wtforms-recaptcha-0.3.2 xmltodict-0.14.2 yagmail-0.15.293 yarl-1.15.2 zipp-3.20.2 zope.deprecation-5.0 zope.interface-7.1.1 zope.sqlalchemy-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlq-y7f7nxdP"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVyCVHdlWSc",
        "outputId": "72f3ed26-cbc1-4ccf-ca56-b07e1236e5e2"
      },
      "source": [
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HS3NNHzmX64"
      },
      "source": [
        "!rm Data/glove.42B.300d.txt\n",
        "!rm Data/glove.42B.300d_w2v.txt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune_client==0.4.107\n",
        "!pip install knockknock==0.1.7\n",
        "!pip install GPUtil==1.4.0\n",
        "!pip install ekphrasis==0.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vI1tnCrV7WM",
        "outputId": "cd1b7f7b-6822-470b-a91b-eaa73b285937"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neptune_client==0.4.107 in /usr/local/lib/python3.10/dist-packages (0.4.107)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (1.0.0)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (0.2.7)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (2.2.2)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (10.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (2.9.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (1.16.0)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (1.8.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune_client==0.4.107) (3.1.43)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune_client==0.4.107) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune_client==0.4.107) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune_client==0.4.107) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune_client==0.4.107) (2024.8.30)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (6.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (3.19.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (1.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado->neptune_client==0.4.107) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune_client==0.4.107) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune_client==0.4.107) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune_client==0.4.107) (2024.2)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from py3nvml->neptune_client==0.4.107) (0.14.2)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (1.1.0)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (4.23.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (3.0.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune_client==0.4.107) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (0.20.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (24.8.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.0.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (6.4.5)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107) (2.9.0.20241003)\n",
            "Requirement already satisfied: knockknock==0.1.7 in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: yagmail>=0.11.214 in /usr/local/lib/python3.10/dist-packages (from knockknock==0.1.7) (0.15.293)\n",
            "Requirement already satisfied: keyring in /usr/lib/python3/dist-packages (from knockknock==0.1.7) (23.5.0)\n",
            "Requirement already satisfied: matrix-client in /usr/local/lib/python3.10/dist-packages (from knockknock==0.1.7) (0.4.0)\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.10/dist-packages (from knockknock==0.1.7) (21.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from knockknock==0.1.7) (2.32.3)\n",
            "Requirement already satisfied: twilio in /usr/local/lib/python3.10/dist-packages (from knockknock==0.1.7) (9.3.6)\n",
            "Requirement already satisfied: premailer in /usr/local/lib/python3.10/dist-packages (from yagmail>=0.11.214->knockknock==0.1.7) (3.10.0)\n",
            "Requirement already satisfied: urllib3~=1.21 in /usr/local/lib/python3.10/dist-packages (from matrix-client->knockknock==0.1.7) (1.26.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->knockknock==0.1.7) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->knockknock==0.1.7) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->knockknock==0.1.7) (2024.8.30)\n",
            "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot->knockknock==0.1.7) (0.27.2)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from twilio->knockknock==0.1.7) (2.9.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from twilio->knockknock==0.1.7) (3.10.10)\n",
            "Requirement already satisfied: aiohttp-retry>=2.8.3 in /usr/local/lib/python3.10/dist-packages (from twilio->knockknock==0.1.7) (2.9.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio->knockknock==0.1.7) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot->knockknock==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot->knockknock==0.1.7) (0.14.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7) (5.3.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.10/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7) (1.2.0)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.10/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7) (2.11.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp>=3.8.4->twilio->knockknock==0.1.7) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.8.4->twilio->knockknock==0.1.7) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx~=0.27->python-telegram-bot->knockknock==0.1.7) (1.2.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cssutils->premailer->yagmail>=0.11.214->knockknock==0.1.7) (10.5.0)\n",
            "Requirement already satisfied: GPUtil==1.4.0 in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: ekphrasis==0.5.1 in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (4.66.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (5.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (3.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (3.8.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (6.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis==0.5.1) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis==0.5.1) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis==0.5.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis==0.5.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis==0.5.1) (2024.9.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis==0.5.1) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ekphrasis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA3eHwn5Xyoe",
        "outputId": "7a5ef002-0bc7-4d7b-fef9-0782cc8531a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Collecting ekphrasis\n",
            "  Using cached ekphrasis-0.5.4-py3-none-any.whl.metadata (610 bytes)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (5.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (6.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.26.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2024.9.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Using cached ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: ekphrasis\n",
            "  Attempting uninstall: ekphrasis\n",
            "    Found existing installation: ekphrasis 0.5.1\n",
            "    Uninstalling ekphrasis-0.5.1:\n",
            "      Successfully uninstalled ekphrasis-0.5.1\n",
            "Successfully installed ekphrasis-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "53QcDc7AqkK5",
        "outputId": "3fdf2d09-4afd-4068-c7d7-78fbf7c6993e"
      },
      "source": [
        "from manual_training_inference import *"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers.modeling_bert'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-04a9809c1776>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmanual_training_inference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/MLProject/manual_training_inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertModels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherModels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MLProject/Models/bertModels.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmasked_cross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSC_weighted_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_bert'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKH2h5hzwcT"
      },
      "source": [
        "path_file='best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file,mode='r') as f:\n",
        "    params = json.load(f)\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "          params[key]=True\n",
        "    elif params[key] == 'False':\n",
        "          params[key]=False\n",
        "    if( key in ['batch_size','num_classes','hidden_size','supervised_layer_pos','num_supervised_heads','random_seed','max_length']):\n",
        "        if(params[key]!='N/A'):\n",
        "            params[key]=int(params[key])\n",
        "\n",
        "    if((key == 'weights') and (params['auto_weights']==False)):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "##### change in logging to output the results to neptune\n",
        "params['logging']='local'\n",
        "params['device']='cpu'\n",
        "params['best_params']=False\n",
        "\n",
        "if torch.cuda.is_available() and params['device']=='cuda':\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Since you dont want to use GPU, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "#### Few handy keys that you can directly change.\n",
        "params['variance']=1\n",
        "params['epochs']=5\n",
        "params['to_save']=True\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "\n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBj47ArF8-F"
      },
      "source": [
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "\n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkvPokoMg3f"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34jLhxdQ5stq"
      },
      "source": [
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SynOxIW5PY-v"
      },
      "source": [
        "!ls explanations_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DVEb9O3IlCd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Ka6ukjTC5M"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-wjKhvNrF5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Bias Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPagtSDQ9zo"
      },
      "source": [
        "from collections import Counter,defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFOmyTVIRJ7R"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw2qaVXLROW4"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'.\n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRrrN8CRRfg"
      },
      "source": [
        "data_all_labelled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14K9-nzFRU8B"
      },
      "source": [
        "def generate_target_information(dataset):\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "\n",
        "    for each in dataset.iterrows():\n",
        "        # All the target communities tagged for this post\n",
        "        all_targets = each[1]['target1']+each[1]['target2']+each[1]['target3']\n",
        "        community_dict = dict(Counter(all_targets))\n",
        "\n",
        "        # Select only those communities which are present more than once.\n",
        "        for key in community_dict:\n",
        "            if community_dict[key]>1:\n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "\n",
        "        # If no community is selected based on majority voting then we don't select any community\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append(key)\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEmm7AD9Ra13"
      },
      "source": [
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPCh_pj3ReFA"
      },
      "source": [
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# We remove None and Other from dictionary\n",
        "community_count_dict.pop('None')\n",
        "community_count_dict.pop('Other')\n",
        "\n",
        "# For the bias calculation, we are considering the top 10 communites based on their count\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "list_selected_community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRU3peSrRhVo"
      },
      "source": [
        "# Based on the top 10 communities, we filter the target_information\n",
        "# This will remove the other communities from the calculation\n",
        "\n",
        "final_target_information ={}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each])&set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsvl0xaRkpw"
      },
      "source": [
        "# Add a new column 'final_target_category' which will contain the selected target community names\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-Y6iMiRnxH"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "postpost_id_divisions_path = './Data/post_id_divisions.json'\n",
        "\n",
        "with open(postpost_id_divisions_path, 'r') as fp:\n",
        "    post_id_dict=json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "data_all_labelled_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT3uzWgcRqaf"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# The names of the files which will contain the bias output for each model\n",
        "bias_score_file_mapping={\n",
        "    'BiRNN-Attn':'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "# The parent folder path of the bias output files\n",
        "parent_path = './explanations_dicts/'\n",
        "\n",
        "# The bias methods that will be considered\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "community_list = list(list_selected_community)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0H9nMZNR1v3"
      },
      "source": [
        "# This function is used to convert the classification into a [0-1] score\n",
        "# with a value of 0 meaning non-toxic and 1 meaning toxic\n",
        "def convert_to_score(label_name, label_dict):\n",
        "    if label_name=='non-toxic':\n",
        "        return 1-label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFyY8iVR4kn"
      },
      "source": [
        "# Function to divide the ids into postive or class class based on the method.\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "    if method=='subgroup':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "            else:\n",
        "                pass\n",
        "    elif method=='bpsn':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "#                 print(eachrow[1]['final_label'])\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    elif method=='bnsp':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        print('Incorrect option selected!!!')\n",
        "\n",
        "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-CBxRFR7YQ"
      },
      "source": [
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "# We load each of the model bias output file and compute the bias score using each method for all the community\n",
        "for each_model in tqdm(bias_score_file_mapping):\n",
        "    total_data ={}\n",
        "    with open(parent_path+bias_score_file_mapping[each_model]) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "\n",
        "            label_to_value = {'toxic':1.0, 'non-toxic':0.0}\n",
        "            for each in community_data['positiveID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "            final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uoz4r8JR-23"
      },
      "source": [
        "%precision 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQi-Am9SCIz"
      },
      "source": [
        "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value =[]\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        print(each_model, each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1-xKrKSFza"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos2FyRyScI6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Calculate Explainability**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0n04ccES0G3"
      },
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ37OLVZS8gB"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess import *\n",
        "from Preprocess.dataCollect import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AZCJG3wS-ko"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class).\n",
        "\n",
        "params = {}\n",
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5DLppxPTAjo"
      },
      "source": [
        "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
        "\n",
        "params_data={\n",
        "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
        "    'bert_tokens':False, #True /False\n",
        "    'type_attention':'softmax', #softmax\n",
        "    'set_decay':0.1,\n",
        "    'majority':2,\n",
        "    'max_length':128,\n",
        "    'variance':10,\n",
        "    'window':4,\n",
        "    'alpha':0.5,\n",
        "    'p_value':0.8,\n",
        "    'method':'additive',\n",
        "    'decay':False,\n",
        "    'normalized':False,\n",
        "    'not_recollect':True,\n",
        "}\n",
        "\n",
        "\n",
        "if(params_data['bert_tokens']):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Loading Normal tokenizer...')\n",
        "    tokenizer=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqwkmy9ITEvH"
      },
      "source": [
        "# Load the whole dataset and get the tokenwise rationales\n",
        "def get_training_data(data):\n",
        "    post_ids_list=[]\n",
        "    text_list=[]\n",
        "    attention_list=[]\n",
        "    label_list=[]\n",
        "\n",
        "    final_binny_output = []\n",
        "    print('total_data',len(data))\n",
        "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
        "        annotation=row['final_label']\n",
        "\n",
        "        text=row['text']\n",
        "        post_id=row['post_id']\n",
        "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
        "        tokens_all = list(row['text'])\n",
        "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
        "\n",
        "        if(annotation!= 'undecided'):\n",
        "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
        "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_binny_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lkIo1ATHjH"
      },
      "source": [
        "training_data=get_training_data(data_all_labelled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxfQUvdTJzn"
      },
      "source": [
        "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    output = []\n",
        "\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if type(each)== int:\n",
        "            start = each\n",
        "            end = each+1\n",
        "        elif len(each) == 2:\n",
        "            start = each[0]\n",
        "            end = each[1]+1\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        output.append({\"docid\":post_id,\n",
        "              \"end_sentence\": -1,\n",
        "              \"end_token\": end,\n",
        "              \"start_sentence\": -1,\n",
        "              \"start_token\": start,\n",
        "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
        "    return output\n",
        "\n",
        "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):\n",
        "    final_output = []\n",
        "\n",
        "    if save_split:\n",
        "        train_fp = open(save_path+'train.jsonl', 'w')\n",
        "        val_fp = open(save_path+'val.jsonl', 'w')\n",
        "        test_fp = open(save_path+'test.jsonl', 'w')\n",
        "\n",
        "    for tcount, eachrow in enumerate(dataset):\n",
        "\n",
        "        temp = {}\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "        majority_label = eachrow[1]\n",
        "\n",
        "        if majority_label=='normal':\n",
        "            continue\n",
        "\n",
        "        all_labels = eachrow[4]\n",
        "        explanations = []\n",
        "        for each_explain in eachrow[3]:\n",
        "            explanations.append(list(each_explain))\n",
        "\n",
        "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
        "        if method == 'union':\n",
        "            final_explanation = [any(each) for each in zip(*explanations)]\n",
        "            final_explanation = [int(each) for each in final_explanation]\n",
        "\n",
        "\n",
        "        temp['annotation_id'] = post_id\n",
        "        temp['classification'] = post_class\n",
        "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
        "        temp['query'] = \"What is the class?\"\n",
        "        temp['query_type'] = None\n",
        "        final_output.append(temp)\n",
        "\n",
        "        if save_split:\n",
        "            if not os.path.exists(save_path+'docs'):\n",
        "                os.makedirs(save_path+'docs')\n",
        "\n",
        "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "\n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp)+'\\n')\n",
        "\n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp)+'\\n')\n",
        "\n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp)+'\\n')\n",
        "            else:\n",
        "                print(post_id)\n",
        "\n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    return final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deYKnU3wTRJn"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "with open('./Data/post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlA0iMeETjUd"
      },
      "source": [
        "!mkdir ./Data/Evaluation\n",
        "!mkdir ./Data/Evaluation/Model_Eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XwjEPOTUaY"
      },
      "source": [
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e76FcTICTXrX"
      },
      "source": [
        "!ls Data/Evaluation/Model_Eval/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9lpwdAeTaf_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWLQB2uT28g"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylHvxsmoUv7Q"
      },
      "source": [
        "cd eraserbenchmark/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajAK6cMUyt6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iudyL6lcXib"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZLhJf-U8Vf"
      },
      "source": [
        "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test  --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json --score_file ../model_explain_output.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1eQENR4VLp_"
      },
      "source": [
        "# print the required results\n",
        "with open('../model_explain_output.json') as fp:\n",
        "    output_data = json.load(fp)\n",
        "\n",
        "print('\\nPlausibility')\n",
        "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
        "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
        "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
        "\n",
        "print('\\nFaithfulness')\n",
        "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
        "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND6DYOMxTU8A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}